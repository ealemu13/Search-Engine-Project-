{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import WindowsPath, Path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a325303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_word_per_url(content):\n",
    "    #given a url returns a dictionary: key-> word, value-> frequency of word\n",
    "    #words in dictionary are stemmed\n",
    "\n",
    "    #https://docs.python.org/3/library/re.html used to with re.findall to find all valid words in url\n",
    "    #https://www.geeksforgeeks.org/python-stemming-words-with-nltk/ used for learining PorterStemmer\n",
    "    # resp = urlopen(url)\n",
    "    # html = resp.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for cur in soup(['script', 'style']):\n",
    "        cur.extract()\n",
    "\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_word_lis = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    frequency = Counter(stemmed_word_lis)\n",
    "\n",
    "    return dict(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(myDict: dict, filename: str):\n",
    "    with open(filename, 'a') as file:\n",
    "        for key in myDict:\n",
    "            data = {key: myDict[key]}\n",
    "            json.dump(data, file)\n",
    "            file.write(\"\\n\")\n",
    "    \n",
    "    print(\"Finished writing to file\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_to_file(myDict: dict, filename: str):\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        for key in myDict:\n",
    "            pos = file.tell()\n",
    "            data = {key: myDict[key]}\n",
    "            json.dump(data, file)\n",
    "            file.write(\"\\n\")\n",
    "    \n",
    "    print(\"Finished writing to file\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e89dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_name, index_dir):\n",
    "\n",
    "    key = 0 # for assigning document id\n",
    "    url_id_map = dict({})   # map each url to an id\n",
    "    inverted_index  = dict({})\n",
    "    max_length = 100000\n",
    "    n_index = 0\n",
    "    n_id = 0\n",
    "    \n",
    "    # read each JSON file, and add url to the map\n",
    "    with open(file_name, \"r\") as file:\n",
    "        for file_path in file:\n",
    "            file_path = WindowsPath(file_path.strip(\"\\n\"))\n",
    "            print(\"Reading content from\", file_path)\n",
    "            \n",
    "            with open(file_path, \"r\") as file:\n",
    "                \n",
    "                # get a json object\n",
    "                jsonObj = json.load(file)\n",
    "        \n",
    "                # get the url and tokens (including its frequency). \n",
    "                url = jsonObj[\"url\"]\n",
    "                content = jsonObj[\"content\"]\n",
    "                tokens = frequency_word_per_url(content)    # key: token, val: frequency\n",
    "                \n",
    "                # map each url to a document id\n",
    "                url_id_map[key] = url\n",
    "        \n",
    "                # update the posting list for each token\n",
    "                for token in tokens:\n",
    "                    if token not in inverted_index:\n",
    "                        inverted_index[token] = {key: tokens[token]}\n",
    "                    else:\n",
    "                        inverted_index[token][key] = tokens[token]\n",
    "                \n",
    "                if len(inverted_index) >= max_length:\n",
    "                    inverted_index = dict(sorted(inverted_index.items()))\n",
    "                    filename = index_dir / f\"inverted_index_{str(n_index)}.jsonl\" \n",
    "                    write_to_file(inverted_index, filename)\n",
    "                    n_index += 1\n",
    "                    inverted_index.clear()\n",
    "\n",
    "                if len(url_id_map) >= max_length:\n",
    "                    filename = index_dir / f\"url_id_{str(n_id)}.jsonl\"\n",
    "                    write_to_file(url_id_map, filename)\n",
    "                    n_id += 1\n",
    "                    url_id_map.clear()\n",
    "\n",
    "        \n",
    "                # update doc id\n",
    "                key += 1\n",
    "\n",
    "        if len(inverted_index) > 0:\n",
    "            inverted_index = dict(sorted(inverted_index.items()))\n",
    "            filename = index_dir / f\"inverted_index_{str(n_index)}.jsonl\"\n",
    "            write_to_file(inverted_index, filename)\n",
    "            n_index += 1\n",
    "\n",
    "        if len(url_id_map) > 0:\n",
    "            filename = index_dir / f\"url_id_{str(n_id)}.jsonl\"\n",
    "            write_to_file(url_id_map, filename)\n",
    "            n_id += 1\n",
    "    \n",
    "    return {\"key\": key, \"n_id\": n_id, \"n_index\": n_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d84d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inverted_index(filename):\n",
    "    inverted_index = dict({})\n",
    "    \n",
    "    with open(filename, \"r\") as file:\n",
    "        for jsonObj in file:\n",
    "            jsonObj = json.loads(jsonObj)\n",
    "            token = list(jsonObj.keys())[0]\n",
    "            inverted_index[token] = jsonObj[token]\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_url_map(url_map: OrderedDict, filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        for jsonObj in file:\n",
    "            jsonObj = json.loads(jsonObj)\n",
    "            token = list(jsonObj.keys())[0]\n",
    "            if len(url_map) > 100000:\n",
    "                url_map.popitem()\n",
    "            url_map[token] = jsonObj[token]\n",
    "    return url_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da036170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_index(filename):\n",
    "    inverted_index = dict({})\n",
    "    \n",
    "    with open(filename, \"r\") as file:\n",
    "        for jsonObj in file:\n",
    "            jsonObj = json.loads(jsonObj)\n",
    "            token = list(jsonObj.keys())[0]\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = jsonObj[token]\n",
    "            else:\n",
    "                existed_postings = Counter(inverted_index[token])\n",
    "                new_postings = Counter(jsonObj[token])\n",
    "                updated_postings = dict(existed_postings + new_postings)\n",
    "                inverted_index[token] = updated_postings\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fe4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_index(filename):\n",
    "    grouped_index = []\n",
    "    alpha_list = [] # contain all the first letters of all current tokens\n",
    "    temp_inverted_index = dict()\n",
    "    last_token = \"\"\n",
    "\n",
    "    # read inverted_index from file\n",
    "    print(\"\\n Read index from file\", filename)\n",
    "    inverted_index = read_inverted_index(filename)\n",
    "\n",
    "    # given a sorted dict, group index by first letter\n",
    "    for token in inverted_index:\n",
    "        if last_token == \"\" or token[0] == last_token[0]:\n",
    "            temp_inverted_index[token] = inverted_index[token]\n",
    "        else:\n",
    "            grouped_index.append(temp_inverted_index.copy())\n",
    "            alpha_list.append(last_token[0])\n",
    "\n",
    "            # reset\n",
    "            temp_inverted_index = dict({})\n",
    "            temp_inverted_index[token] = inverted_index[token]\n",
    "        \n",
    "        last_token = token\n",
    "\n",
    "    # save the remaining to grouped_index\n",
    "    if len(temp_inverted_index) > 0:\n",
    "        grouped_index.append(temp_inverted_index.copy())\n",
    "        alpha_list.append(last_token[0])\n",
    "    \n",
    "    # write index to new files\n",
    "    index_dir = Path(\"inverted_index\") # folder's name to save index files\n",
    "    index_dir.mkdir(exist_ok=True)\n",
    "    for i in range(len(grouped_index)):\n",
    "        char = alpha_list[i]\n",
    "        filename = index_dir / f\"{char}.jsonl\"\n",
    "        write_to_file(grouped_index[i], filename)\n",
    "\n",
    "    return alpha_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ded13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cache_index(cache_inverted_index: OrderedDict, token):\n",
    "    filename = \"offset/\" + token[0] + \".jsonl\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        for jsonObj in file:\n",
    "            jsonObj = json.loads(jsonObj)\n",
    "            term = list(jsonObj.keys())[0]\n",
    "\n",
    "            if len(cache_inverted_index) > 100000:\n",
    "                cache_inverted_index.popitem()\n",
    "            cache_inverted_index[term] = jsonObj[term]\n",
    "    return cache_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posting(token, pos):\n",
    "    filename = \"final_inverted_index/\" + token[0] + \".jsonl\"\n",
    "    posting = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        file.seek(pos)\n",
    "        jsonObj = file.readline()\n",
    "        jsonObj = json.loads(jsonObj)\n",
    "        posting = jsonObj[token]\n",
    "    return posting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a search engine\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "def search_engine(queries, total_urls):\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    max_length = 100000\n",
    "    cache_offset_index = OrderedDict()    # recently searched items are moved to the back\n",
    "\n",
    "    # perform the search\n",
    "    # cristina lopes, machine learning, ACM, master of software engineering\n",
    "\n",
    "    tokens = queries.split() # ex: \"machine learning\" => ['machine', 'learning']\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "    posting_lists = []      # for saving all the url id \n",
    "    inverted_index = dict() # for calculating tf-idf score purpose\n",
    "\n",
    "    # find the posting lists of each token\n",
    "    for token in stemmed_tokens:\n",
    "        if token not in cache_offset_index:\n",
    "            cache_offset_index = update_cache_index(cache_offset_index, token)\n",
    "\n",
    "        if token in cache_offset_index:\n",
    "            posting = get_posting(token, int(cache_offset_index[token]))\n",
    "            posting_lists.append(posting)\n",
    "            cache_offset_index.move_to_end(token)\n",
    "            inverted_index[token] = posting\n",
    "\n",
    "        # Boolean AND: intersect all posting lists\n",
    "        url_id_set = set()\n",
    "        for posting_list in posting_lists:\n",
    "            doc_ids = set(posting_list.keys())\n",
    "            if not url_id_set:\n",
    "                url_id_set = doc_ids\n",
    "            else:\n",
    "                url_id_set &= doc_ids\n",
    "\n",
    "        # compute tf-idf scores\n",
    "        doc_scores = {}\n",
    "        N = int(total_urls)\n",
    "\n",
    "        for doc_id in url_id_set:\n",
    "            score = 0.0\n",
    "            for token in stemmed_tokens:\n",
    "                posting = inverted_index.get(token, {})\n",
    "                tf = posting.get(doc_id, 0)\n",
    "                df = len(posting) if posting else 1\n",
    "                idf = math.log(N / (1 + df))\n",
    "                score += tf * idf\n",
    "            doc_scores[doc_id] = score\n",
    "            \n",
    "        ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return url_id_set, ranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1203aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run collect_file_path.py to get the txt file (named \"JSON_file_path.txt\")\n",
    "file_name = \"JSON_file_path.txt\"\n",
    "\n",
    "# create inverted_index, return number of index files and url_id files\n",
    "index_dir = Path(\"jsonl_files\") # folder's name to save index files\n",
    "index_dir.mkdir(exist_ok=True) \n",
    "return_dict = create_inverted_index(file_name, index_dir)\n",
    "# return_dict = {\"key\": 55393, \"n_id\": 1, \"n_index\": 28}  # for developing mode\n",
    "\n",
    "# save return values to a file\n",
    "write_to_file(return_dict, \"return_values.txt\")\n",
    "\n",
    "# read files in json_files, group index by first letters, return a list of letters\n",
    "# new inverted_index files are under inverted_index folder\n",
    "num_of_index_files = return_dict[\"n_index\"]\n",
    "chars = set([])\n",
    "for i in range(num_of_index_files):\n",
    "    filename = index_dir / f\"inverted_index_{i}.jsonl\"\n",
    "    letters = group_index(filename)\n",
    "    chars.update(letters)\n",
    "\n",
    "# for each inverted_index/{char} file\n",
    "# reorganzie, merge/update the token postings\n",
    "index_dir = Path(\"final_inverted_index\") # folder's name to save index files\n",
    "index_dir.mkdir(exist_ok=True) \n",
    "for char in chars:\n",
    "    filename = \"inverted_index/\" + char + \".jsonl\"\n",
    "    newFilename = \"final_\" + filename\n",
    "    inverted_index = merge_index(filename)\n",
    "    rewrite_to_file(inverted_index, newFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve the indexer by saving the position of each token in final_inverted_index' files\n",
    "offset_dir = Path(\"offset\") # folder's name to save index files\n",
    "offset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename in Path(\"final_inverted_index\").iterdir():\n",
    "    position_dict = dict()\n",
    "    with open(filename, 'r') as file:\n",
    "        while True:\n",
    "            pos = file.tell()\n",
    "            line = file.readline()\n",
    "            if not line: break\n",
    "\n",
    "            jsonObj = json.loads(line)\n",
    "            token = list(jsonObj.keys())[0]\n",
    "            position_dict[token] = pos\n",
    "    \n",
    "    offset_filename = offset_dir / str(filename.name)\n",
    "    with open(offset_filename, 'a') as file:\n",
    "        for token in position_dict:\n",
    "            data = {token: position_dict[token]}\n",
    "            json.dump(data, file)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"jsonl_files/url_id_0.jsonl\"\n",
    "url_map = OrderedDict()\n",
    "update_url_map(url_map, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_of_urls = return_dict[\"key\"]\n",
    "# num_of_urls = 55393 (for dev mode)\n",
    "\n",
    "# perform the search\n",
    "url_map = dict()\n",
    "while True:\n",
    "\n",
    "    # ask for input/queries\n",
    "    queries = input(\"Input queries (type exit to stop searching): \")\n",
    "    if queries == \"exit\".lower():\n",
    "        break\n",
    "    else:\n",
    "        startTime = time.process_time_ns()\n",
    "        \n",
    "        url_id_set, ranked_docs = search_engine(queries, num_of_urls)\n",
    "\n",
    "        # get url_id map\n",
    "        for id in url_id_set:\n",
    "            if id not in url_map:\n",
    "                filename = f\"jsonl_files/url_id_{int(id) // 100000}.jsonl\"\n",
    "                url_map = update_url_map(url_map, filename)\n",
    "\n",
    "        endTime = time.process_time_ns()\n",
    "\n",
    "        # Output up to 5 URLs (no ranking yet)\n",
    "        print(f\"\\nURLs that contain: \\\"{queries}\\\"\")\n",
    "        for i, (doc_id, score) in enumerate(ranked_docs[:5]):\n",
    "            url = url_map.get(doc_id, doc_id)\n",
    "            print(f\"{i+1}. {url} (tf-idf: {score:.4f})\")\n",
    "        print(\"Time Response:\", (endTime-startTime) / 10**6, \"ms\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
