{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines, json\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "path = [\"data/inverted_index_1.jsonl\", \"data/inverted_index_2.jsonl\"]\n",
    "inverted_index = {}\n",
    "url_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280094fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the jsonl file to save the inverted index to a map\n",
    "with jsonlines.open(path[0], 'r') as file:\n",
    "    for jsonObj in file:\n",
    "        token = list(jsonObj.keys())[0]\n",
    "        inverted_index[token] = jsonObj[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922886b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the jsonl files to save the inverted index to a map\n",
    "with jsonlines.open(path[1], 'r') as file:\n",
    "    for jsonObj in file:\n",
    "        token = list(jsonObj.keys())[0]\n",
    "        inverted_index[token] = jsonObj[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purpose\n",
    "# i = 0\n",
    "# for key in inverted_index:\n",
    "#     print(key, inverted_index[key])\n",
    "#     i += 1\n",
    "\n",
    "#     if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file to save the urls' id to a map\n",
    "with open(\"url_id.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for id in data:\n",
    "        url_map[id] = data[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adcabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purpose \n",
    "# i = 0\n",
    "# for key in url_map:\n",
    "#     print(key, url_map[key])\n",
    "#     i += 1\n",
    "\n",
    "#     if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    posting_lists = []      # for saving all the url id \n",
    "    searching_tokens = [] \n",
    "    \n",
    "    # ask for input/quries\n",
    "    queries = input(\"Input queries (type exit to stop searching): \")\n",
    "\n",
    "    if queries.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # perform the search\n",
    "    # cristina lopes, machine learning, ACM, master of software engineering\n",
    "\n",
    "    searching_tokens = queries.split()  # ex: \"machine learning\" => ['machine', 'learning']\n",
    "\n",
    "    # note: need a stemming step here before start searching\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_searching_tokens = [stemmer.stem(word) for word in searching_tokens]\n",
    "\n",
    "    # get the posting list from each token and save to posting_lists array\n",
    "    for token in stemmed_searching_tokens:\n",
    "        if token in inverted_index:\n",
    "            posting_lists.append(inverted_index[token])\n",
    "        else:\n",
    "            print(token, \"is not found\")\n",
    "\n",
    "    # find the intersection of the posting lists\n",
    "    url_id_set = set()\n",
    "    for posting_list in posting_lists:\n",
    "        if len(url_id_set) == 0:\n",
    "            url_id_set.update(set(posting_list.keys()))\n",
    "        else:\n",
    "            # url_id_set.intersection(set(posting_list.keys()))\n",
    "            url_id_set &= set(posting_list.keys())\n",
    "\n",
    "    # output the urls\n",
    "    i = 0\n",
    "    print(\"URLs that contain\", queries)\n",
    "    for id in url_id_set:\n",
    "        if i > 5: break\n",
    "        print(url_map[id])\n",
    "        i += 1\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk jsonlines\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import jsonlines, json\n",
    "import math\n",
    "\n",
    "N = 55393 # total number of documents\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load inverted index\n",
    "path = [\"data/inverted_index_1.jsonl\", \"data/inverted_index_2.jsonl\"]\n",
    "inverted_index = {}\n",
    "\n",
    "for p in path:\n",
    "    with jsonlines.open(p, 'r') as file:\n",
    "        for jsonObj in file:\n",
    "            token = list(jsonObj.keys())[0]\n",
    "            inverted_index[token] = jsonObj[token]\n",
    "\n",
    "# Load URL map\n",
    "url_map = {}\n",
    "with open(\"url_id.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for id in data:\n",
    "        url_map[id] = data[id]\n",
    "\n",
    "# Query loop\n",
    "while True:\n",
    "    queries = input(\"Input queries (type exit to stop searching): \")\n",
    "    if queries.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    tokens = queries.split()\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    posting_lists = []\n",
    "    for token in stemmed_tokens:\n",
    "        if token in inverted_index:\n",
    "            posting_lists.append(inverted_index[token])\n",
    "        else:\n",
    "            print(token, \"is not found\")\n",
    "\n",
    "    # Boolean AND: intersect all posting lists\n",
    "    url_id_set = set()\n",
    "    for posting_list in posting_lists:\n",
    "        doc_ids = set(posting_list.keys())\n",
    "        if not url_id_set:\n",
    "            url_id_set = doc_ids\n",
    "        else:\n",
    "            url_id_set &= doc_ids\n",
    "\n",
    "    # compute tf-idf scores\n",
    "    doc_scores = {}\n",
    "\n",
    "    for doc_id in url_id_set:\n",
    "        score = 0.0\n",
    "        for token in stemmed_tokens:\n",
    "            posting = inverted_index.get(token, {})\n",
    "            tf = posting.get(doc_id, 0)\n",
    "            df = len(posting) if posting else 1\n",
    "            idf = math.log(N / (1 + df))\n",
    "            score += tf * idf\n",
    "        doc_scores[doc_id] = score\n",
    "\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Output up to 5 URLs (no ranking yet)\n",
    "    print(f\"\\nURLs that contain: \\\"{queries}\\\"\")\n",
    "    for i, (doc_id, score) in enumerate(ranked_docs[:5]):\n",
    "        url = url_map.get(doc_id, doc_id)\n",
    "        print(f\"{i+1}. {url} (tf-idf: {score:.4f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f55c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3a952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca13ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
